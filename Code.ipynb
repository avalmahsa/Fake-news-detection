{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K-3xqDS-T1j"
   },
   "outputs": [],
   "source": [
    "# libriaries used are loaded\n",
    "\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D, Bidirectional\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRQc_xPN-q1j",
    "outputId": "c63b3990-af11-4b1e-9e53-ff2c9d37d65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# grab dataset from google drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "xy_train_df = pd.read_csv('/content/drive/My Drive/xy_train.csv')\n",
    "x_test_df = pd.read_csv('/content/drive/My Drive/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5eq0_6oNHnfi"
   },
   "outputs": [],
   "source": [
    "x = xy_train_df.text\n",
    "y = xy_train_df.label\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OZtTMREGV7N"
   },
   "outputs": [],
   "source": [
    "# maximum number of words from the resulting tokenized data which are to be used\n",
    "vocab_size = 40000\n",
    "max_len = 40\n",
    "\n",
    "\n",
    "# build vocabulary from training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "# upadting the internal vocalulary based on the list of text, so it creates the vocabulary\n",
    "# index based on word frequnecy so every word gets a unique interger value so lower integers \n",
    "# mean more frequent word.\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "def _preprocess(list_of_text):\n",
    "    # pads sequence to the same length (all sequences in a list to have the same length), it\n",
    "    # does so by padding 0 in the beggining of each sequence until they have the same length as\n",
    "    # the longest sequence. \n",
    "    return pad_sequences(\n",
    "        # transforms each text in texts to a sequence of integers. It takes each word\n",
    "        # in the text and replaces it with its corresponding integer value from the \n",
    "        # dictionary.\n",
    "        tokenizer.texts_to_sequences(list_of_text),\n",
    "        # takes in the pre-defined input (40) as maximum length of all sequences.\n",
    "        maxlen=max_len,\n",
    "        # does padding after each sequence\n",
    "        padding='post',\n",
    "    )\n",
    "    \n",
    "\n",
    "# padding is done inside: \n",
    "x_train = _preprocess(x_train)\n",
    "x_valid = _preprocess(x_valid)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8Zy7IseZemv"
   },
   "outputs": [],
   "source": [
    "# Stop word removal before passing it to preprocessing\n",
    "x = xy_train_df.text\n",
    "def filter_stop_words(x, stop_words):\n",
    "    for i, sentence in enumerate(x):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        x[i] = ' '.join(new_sent)\n",
    "    return x\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "train_sentences = filter_stop_words(x, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiYRltnQSp7b"
   },
   "outputs": [],
   "source": [
    "# Preprocessing to include stop word removal\n",
    "\n",
    "y = xy_train_df.label\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)v\n",
    "\n",
    "vocab_size = 40000\n",
    "max_len = 40\n",
    "\n",
    "\n",
    "# build vocabulary from training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "\n",
    "def _preprocess(train_sentences):\n",
    "    return pad_sequences(\n",
    "        tokenizer.texts_to_sequences(train_sentences),\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "    )\n",
    "    \n",
    "\n",
    "# padding is done inside: \n",
    "x_train = _preprocess(x_train)\n",
    "x_valid = _preprocess(x_valid)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWzi8kyOLwiZ"
   },
   "outputs": [],
   "source": [
    "# these match as we can see. It has 23 words and the rest is padded to match 40 which was a \n",
    "# predefined argument. \n",
    "print(x_train[:4])\n",
    "pprint(tokenizer.sequences_to_texts(x_train[:4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERrgEy255JL4"
   },
   "outputs": [],
   "source": [
    "# FULLY CONNECTED NETWORK (template version with comments)\n",
    "\n",
    "\n",
    "# defines an input layer (instantiate a Keras tensor object) and allows for building a model.\n",
    "# batch_shape basically means shape=(40,), this is useful for LSTMs as it lets you keppt the hidden state\n",
    "# values in an LSTM across batches.\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "# this layer can only be used as the first layer in a model. This is the first hidden layer of a \n",
    "# network and will learn an embessing for all of the words in the trainin dataset. Here we are giving \n",
    "# it input and output integers\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "# computes the mean of elements across dimensions of the vector. It reduced the input variables along\n",
    "# the dimenions given in axis by computing the mean of elements across dimensions in the axis. Here in\n",
    "# the provided code we are reducing the dimensions by one.\n",
    "averaged = tf.reduce_mean(embedded, axis=1)\n",
    "# just your everyday dense layer with a activation template parameter of sigmoid which transforms the\n",
    "# input into a value between 0 and 1. \n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(averaged)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=35,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lEU1cUvfZvn"
   },
   "outputs": [],
   "source": [
    "# FULLY CONNECTED NETWORK (without comments, easier to see)\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "averaged = tf.reduce_mean(embedded, axis=1)\n",
    "d1 = keras.layers.Dense(200)(averaged)\n",
    "do1 = keras.layers.Dropout(0.2)(d1)\n",
    "d2 = keras.layers.Dense(300)(do1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(d2)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    #optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    #optimizer=tf.keras.optimizers.Adagrad(),\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=70,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJ8dgpAnqZ4c"
   },
   "outputs": [],
   "source": [
    "# Recurrent GRU\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "gru1 = GRU(units = 110, dropout = 0.2, recurrent_dropout = 0.2)(embedded)\n",
    "do1 = Dropout(rate = 0.2)(gru1)\n",
    "#d1 = keras.layers.Dense(100)(do1)\n",
    "#do2 = Dropout(rate = 0.2)(d1)\n",
    "pred = keras.layers.Dense(1)(do1)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=44,\n",
    "                    validation_data=(x_valid, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcB2mpPszlug"
   },
   "outputs": [],
   "source": [
    "# Recurrent LSTM\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "lstm1 = LSTM(units = 100, dropout = 0.2, recurrent_dropout = 0.2, )(embedded)\n",
    "do1 = Dropout(rate = 0.4)(lstm1)\n",
    "d1 = keras.layers.Dense(100)(do1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(d1)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=27,\n",
    "                    batch_size=65,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3x2cx7M0jMC"
   },
   "outputs": [],
   "source": [
    "# Recurrent GRU Multi-layer\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "gru1 = GRU(units = 100, dropout = 0.2, recurrent_dropout = 0.2,return_sequences=True)(embedded)\n",
    "do1 = Dropout(rate = 0.2)(gru1)\n",
    "gru2 = GRU(units = 100,return_sequences=True)(do1)\n",
    "do2 = Dropout(rate = 0.2)(gru2)\n",
    "d1 = keras.layers.Dense(100)(do2)\n",
    "do3 = Dropout(rate = 0.2)(d1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(do3)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=65,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tIg1JQz2nGx"
   },
   "outputs": [],
   "source": [
    "# Recurrent LSTM Multi-layer\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "lstm1 = LSTM(units = 100, dropout = 0.2, recurrent_dropout = 0.2, return_sequences=True)(embedded)\n",
    "do1 = Dropout(rate = 0.2)(lstm1)\n",
    "lstm2 = LSTM(units = 100, dropout = 0.2, recurrent_dropout = 0.2, return_sequences=True)(do1)\n",
    "do2 = Dropout(rate = 0.2)(lstm2)\n",
    "d1 = keras.layers.Dense(100)(do2)\n",
    "do3 = Dropout(rate = 0.2)(d1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(do3)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=65,\n",
    "                    validation_data=(x_valid, y_valid))\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAT6L6xTZPJK"
   },
   "outputs": [],
   "source": [
    "# Bi_directional Recurrent GRU\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "bgru1 = Bidirectional(GRU(units = 100, dropout = 0.2, recurrent_dropout = 0.2))(embedded)\n",
    "do1 = Dropout(rate = 0.2)(bgru1)\n",
    "d1 = keras.layers.Dense(100)(do1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(d1)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=60,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L3aWLB3CAKp"
   },
   "outputs": [],
   "source": [
    "# Bi_directional Recurrent LSTM\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "#averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "bgru1 = Bidirectional(LSTM(units = 100, dropout = 0.2, recurrent_dropout = 0.2))(embedded)\n",
    "do1 = Dropout(rate = 0.2)(bgru1)\n",
    "d1 = keras.layers.Dense(100)(do1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(d1)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=60,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "HfEDzXRtCkA-"
   },
   "outputs": [],
   "source": [
    "x_test = _preprocess(x_test_df.text)\n",
    "y_predict = np.squeeze(model.predict(x_test))\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'label':y_predict}).to_csv('RNN_biLSTM_stopword.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
